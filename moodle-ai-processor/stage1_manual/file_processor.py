"""
Stage 1: Manual File Processing

This module processes uploaded HTML and Word documents from Moodle
using AI without direct API integration.
"""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import re

from bs4 import BeautifulSoup
import docx
import sys
sys.path.append(str(Path(__file__).parent.parent / 'src'))
from src.ai_client import OpenRouterClient


class FileProcessor:
    """Process uploaded HTML and Word documents"""
    
    def __init__(self, config_path: str = "config/config.json"):
        """Initialize the file processor"""
        # Handle both relative and absolute config paths
        if not Path(config_path).exists():
            config_path = Path(__file__).parent.parent / config_path
        
        self.config_path = config_path
        self.config = self._load_config()
        self.ai_client = OpenRouterClient(
            api_key=self.config['openrouter']['api_key'],
            base_url=self.config['openrouter']['base_url'],
            model=self.config['openrouter']['model']
        )
        
        # Set paths relative to the project root
        project_root = Path(__file__).parent.parent
        self.upload_dir = project_root / "stage1_manual/uploads"
        self.processed_dir = project_root / "stage1_manual/processed"
        self.logger = logging.getLogger(__name__)
        
        # Ensure directories exist
        self.upload_dir.mkdir(parents=True, exist_ok=True)
        self.processed_dir.mkdir(parents=True, exist_ok=True)
    
    def _load_config(self) -> Dict[str, Any]:
        """Load configuration from JSON file"""
        with open(self.config_path, 'r') as f:
            return json.load(f)
    
    def extract_text_from_html(self, html_content: str) -> Dict[str, Any]:
        """Extract text and structure from HTML"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Extract forum posts if it's a forum page
        posts = []
        
        # Common Moodle forum post selectors
        post_elements = soup.find_all(['div', 'article'], class_=re.compile(r'post|discussion|forum'))
        
        if not post_elements:
            # Fallback: look for any content that might be posts
            post_elements = soup.find_all(['div', 'p'], string=re.compile(r'.{50,}'))
        
        for element in post_elements:
            post_text = element.get_text().strip()
            if len(post_text) > 50:  # Filter out very short content
                # Try to extract author and timestamp
                author = self._extract_author(element)
                timestamp = self._extract_timestamp(element)
                
                posts.append({
                    'content': post_text,
                    'author': author,
                    'timestamp': timestamp,
                    'html_class': element.get('class', [])
                })
        
        # If no posts found, extract all meaningful text
        if not posts:
            text = soup.get_text()
            # Clean up whitespace
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            posts.append({
                'content': text,
                'author': 'Unknown',
                'timestamp': None,
                'html_class': []
            })
        
        return {
            'posts': posts,
            'total_posts': len(posts),
            'extraction_method': 'html_parsing'
        }
    
    def _extract_author(self, element) -> str:
        """Try to extract author from HTML element"""
        # Look for common author indicators
        author_element = element.find(['span', 'div', 'p'], class_=re.compile(r'author|user|name'))
        if author_element:
            return author_element.get_text().strip()
        
        # Look for "by" patterns
        text = element.get_text()
        by_match = re.search(r'by\s+([^,\n]+)', text, re.IGNORECASE)
        if by_match:
            return by_match.group(1).strip()
        
        return 'Unknown'
    
    def _extract_timestamp(self, element) -> Optional[str]:
        """Try to extract timestamp from HTML element"""
        # Look for time elements
        time_element = element.find('time')
        if time_element:
            return time_element.get('datetime') or time_element.get_text().strip()
        
        # Look for date patterns
        text = element.get_text()
        date_patterns = [
            r'\d{1,2}/\d{1,2}/\d{4}',
            r'\d{4}-\d{2}-\d{2}',
            r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},\s+\d{4}'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        
        return None
    
    def extract_text_from_docx(self, file_path: Path) -> Dict[str, Any]:
        """Extract text from Word document"""
        try:
            doc = docx.Document(file_path)
            
            paragraphs = []
            for para in doc.paragraphs:
                if para.text.strip():
                    paragraphs.append({
                        'content': para.text.strip(),
                        'style': para.style.name if para.style else 'Normal'
                    })
            
            # Also extract from tables
            tables_content = []
            for table in doc.tables:
                for row in table.rows:
                    row_data = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_data.append(cell.text.strip())
                    if row_data:
                        tables_content.append(row_data)
            
            return {
                'paragraphs': paragraphs,
                'tables': tables_content,
                'total_paragraphs': len(paragraphs),
                'total_tables': len(tables_content),
                'extraction_method': 'docx_parsing'
            }
        except Exception as e:
            self.logger.error(f"Error extracting from docx: {e}")
            return {'error': str(e)}
    
    def process_file(self, file_path: Path, processing_type: str = 'feedback') -> Dict[str, Any]:
        """Process a single file with AI"""
        print(f"\nğŸ“„ Processing file: {file_path.name}")
        print(f"   Type: {processing_type}")
        print(f"   Size: {file_path.stat().st_size:,} bytes")
        
        result = {
            'file_name': file_path.name,
            'file_path': str(file_path),
            'processed_at': datetime.now().isoformat(),
            'processing_type': processing_type
        }
        
        try:
            # Extract content based on file type
            if file_path.suffix.lower() == '.html':
                print("   ğŸ” Extracting content from HTML...")
                with open(file_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                extracted_data = self.extract_text_from_html(html_content)
                result['extraction'] = extracted_data
                print(f"   âœ… Found {extracted_data['total_posts']} posts/sections")
                
                # Process each post
                processed_posts = []
                for i, post in enumerate(extracted_data['posts'], 1):
                    print(f"\n   ğŸ“ Processing post {i}/{len(extracted_data['posts'])}")
                    print(f"      Author: {post['author']}")
                    print(f"      Length: {len(post['content'])} chars")
                    
                    ai_response = self._process_content_with_ai(
                        content=post['content'],
                        author=post['author'],
                        processing_type=processing_type
                    )
                    processed_posts.append({
                        'original': post,
                        'ai_response': ai_response
                    })
                    print(f"   âœ… Post {i} processed successfully")
                
                result['processed_posts'] = processed_posts
                print(f"\nâœ… HTML processing complete: {len(processed_posts)} posts processed")
                
            elif file_path.suffix.lower() in ['.docx', '.doc']:
                print("   ğŸ” Extracting content from Word document...")
                extracted_data = self.extract_text_from_docx(file_path)
                result['extraction'] = extracted_data
                
                if 'error' not in extracted_data:
                    print(f"   âœ… Found {extracted_data['total_paragraphs']} paragraphs")
                    # Combine all paragraphs for AI processing
                    full_content = '\n\n'.join([p['content'] for p in extracted_data['paragraphs']])
                    
                    print(f"   ğŸ“ Processing document content ({len(full_content)} chars)...")
                    ai_response = self._process_content_with_ai(
                        content=full_content,
                        processing_type=processing_type
                    )
                    result['ai_response'] = ai_response
                    print("   âœ… Document processing complete")
                else:
                    print(f"   âŒ Error extracting content: {extracted_data['error']}")
            
            else:
                print("   ğŸ” Processing as plain text...")
                # Try to read as plain text
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                print(f"   ğŸ“ Processing text content ({len(content)} chars)...")
                ai_response = self._process_content_with_ai(
                    content=content,
                    processing_type=processing_type
                )
                result['ai_response'] = ai_response
                print("   âœ… Text processing complete")
        
        except Exception as e:
            result['error'] = str(e)
            self.logger.error(f"Error processing {file_path}: {e}")
        
        return result
    
    def _process_content_with_ai(self, content: str, author: str = None, 
                                processing_type: str = 'feedback') -> Dict[str, Any]:
        """Process content with AI based on processing type"""
        try:
            print(f"ğŸ¤– Processing with AI (type: {processing_type})...")
            if author:
                print(f"   Author: {author}")
            print(f"   Content length: {len(content)} characters")
            
            if processing_type == 'feedback':
                print("   ğŸ” Analyzing student post...")
                analysis = self.ai_client.analyze_student_post(content)
                print("   âœ… Analysis complete")
                
                print("   ğŸ’¬ Generating reply...")
                reply = self.ai_client.generate_reply(content, reply_style="supportive")
                print("   âœ… Reply generated")
                
                return {
                    'type': 'feedback',
                    'analysis': analysis,
                    'suggested_reply': reply,
                    'author': author
                }
            
            elif processing_type == 'summary':
                print("   ğŸ“‹ Generating summary...")
                summary_prompt = f"Please provide a concise summary of this student work:\n\n{content}"
                summary = self.ai_client.generate_response(
                    summary_prompt,
                    system_prompt="You are an educational assistant. Provide clear, concise summaries."
                )
                print("   âœ… Summary complete")
                
                return {
                    'type': 'summary',
                    'summary': summary,
                    'author': author
                }
            
            elif processing_type == 'grading':
                print("   ğŸ“Š Generating grading feedback...")
                grading_prompt = f"""Please provide grading feedback for this student work:

{content}

Provide:
1. Strengths of the work
2. Areas for improvement  
3. Suggested grade/score (explain your reasoning)
4. Specific recommendations"""
                
                grading = self.ai_client.generate_response(
                    grading_prompt,
                    system_prompt="You are an experienced educator providing constructive grading feedback."
                )
                print("   âœ… Grading feedback complete")
                
                return {
                    'type': 'grading',
                    'grading_feedback': grading,
                    'author': author
                }
            
        except Exception as e:
            return {
                'type': processing_type,
                'error': str(e)
            }
    
    def process_upload_directory(self, processing_type: str = 'feedback') -> Dict[str, Any]:
        """Process all files in the upload directory"""
        upload_path = Path(self.upload_dir)
        results = []
        
        print(f"\nğŸš€ Starting batch processing (type: {processing_type})")
        print(f"ğŸ“ Upload directory: {upload_path}")
        
        # Get all supported files
        supported_extensions = ['.html', '.htm', '.docx', '.doc', '.txt']
        files_to_process = []
        
        for ext in supported_extensions:
            files_to_process.extend(upload_path.glob(f'*{ext}'))
        
        if not files_to_process:
            print("âŒ No supported files found in upload directory")
            print(f"   Supported formats: {', '.join(supported_extensions)}")
            return {'error': 'No files to process'}
        
        print(f"ğŸ“‹ Found {len(files_to_process)} files to process")
        for i, file_path in enumerate(files_to_process, 1):
            print(f"   {i}. {file_path.name} ({file_path.stat().st_size:,} bytes)")
        
        print(f"\nâš¡ Processing files...")
        
        for i, file_path in enumerate(files_to_process, 1):
            print(f"\n{'='*50}")
            print(f"ğŸ“„ Processing file {i}/{len(files_to_process)}")
            print(f"{'='*50}")
            
            result = self.process_file(file_path, processing_type)
            results.append(result)
            
            # Save individual result
            result_file = self.processed_dir / f"{file_path.stem}_{processing_type}_result.json"
            print(f"ğŸ’¾ Saving results to: {result_file.name}")
            with open(result_file, 'w') as f:
                json.dump(result, f, indent=2, default=str)
            
            if 'error' in result:
                print(f"âŒ Error processing {file_path.name}: {result['error']}")
            else:
                print(f"âœ… Successfully processed {file_path.name}")
        
        # Save combined results
        print(f"\nğŸ“Š Generating combined results...")
        combined_result = {
            'processing_type': processing_type,
            'processed_at': datetime.now().isoformat(),
            'total_files': len(files_to_process),
            'results': results
        }
        
        combined_file = self.processed_dir / f"combined_{processing_type}_results.json"
        print(f"ğŸ’¾ Saving combined results to: {combined_file.name}")
        with open(combined_file, 'w') as f:
            json.dump(combined_result, f, indent=2, default=str)
        
        # Final summary
        success_count = len([r for r in results if 'error' not in r])
        error_count = len(files_to_process) - success_count
        
        print(f"\nğŸ‰ Batch processing complete!")
        print(f"   âœ… Successfully processed: {success_count}")
        if error_count > 0:
            print(f"   âŒ Errors: {error_count}")
        print(f"   ğŸ“ Results saved to: {self.processed_dir}")
        
        return combined_result
